{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2E2S Model Training on Kaggle\n",
    "## Dual-channel Enhanced Entity-Sentiment Model for Aspect-Based Sentiment Analysis\n",
    "\n",
    "This notebook provides a complete setup for training the D2E2S model on Kaggle with optimal hyperparameters.\n",
    "\n",
    "**Dataset**: 15res (Restaurant Reviews 2015)\n",
    "**Optimal Configuration**: Hybrid GCN with production-ready hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch==2.4.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install torch-geometric==2.3.1\n",
    "!pip install transformers==4.28.1\n",
    "!pip install tensorboardX==2.6\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install spacy>=3.7.2,<3.8.0\n",
    "!pip install matplotlib==3.7.1\n",
    "!pip install pydantic>=2.7.0\n",
    "!pip install pandas>=1.3.0\n",
    "!pip install seaborn>=0.11.0\n",
    "\n",
    "# Download spacy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.optim import optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, AutoTokenizer, AutoConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload and Extract Dataset\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload your D2E2S dataset as a ZIP file to Kaggle\n",
    "2. The dataset should contain the following structure:\n",
    "   - `data/15res/train_dep_triple_polarity_result.json`\n",
    "   - `data/15res/test_dep_triple_polarity_result.json`\n",
    "   - `data/types.json`\n",
    "   - All model files in `models/` directory\n",
    "   - All trainer files in `trainer/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset (adjust path according to your uploaded file)\n",
    "import zipfile\n",
    "\n",
    "# Assuming you uploaded the dataset as 'd2e2s_dataset.zip'\n",
    "dataset_path = '/kaggle/input/d2e2s-dataset/d2e2s_dataset.zip'  # Adjust this path\n",
    "\n",
    "# Extract dataset\n",
    "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/kaggle/working/')\n",
    "\n",
    "# Verify extraction\n",
    "print(\"Dataset extracted. Contents:\")\n",
    "for root, dirs, files in os.walk('/kaggle/working/'):\n",
    "    level = root.replace('/kaggle/working/', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files only\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Optimal Configuration\n",
    "\n",
    "Using the best hyperparameters from Optuna optimization for 15res dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal hyperparameters for 15res dataset (F1: 0.8644)\n",
    "OPTIMAL_CONFIG = {\n",
    "    \"batch_size\": 6,\n",
    "    \"lr\": 0.000312,\n",
    "    \"lr_warmup\": 0.12,\n",
    "    \"weight_decay\": 0.0012,\n",
    "    \"gcn_type\": \"hybrid\",\n",
    "    \"gcn_layers\": 2,\n",
    "    \"attention_heads\": 16,\n",
    "    \"hidden_dim\": 1024,\n",
    "    \"gcn_dim\": 768,\n",
    "    \"epochs\": 40\n",
    "}\n",
    "\n",
    "print(\"Optimal Configuration:\")\n",
    "for key, value in OPTIMAL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Dataset configuration\n",
    "        self.dataset = \"15res\"\n",
    "        self.dataset_file = {\n",
    "            \"train\": \"/kaggle/working/data/15res/train_dep_triple_polarity_result.json\",\n",
    "            \"test\": \"/kaggle/working/data/15res/test_dep_triple_polarity_result.json\",\n",
    "            \"types_path\": \"/kaggle/working/data/types.json\"\n",
    "        }\n",
    "        \n",
    "        # Optimal hyperparameters\n",
    "        self.batch_size = OPTIMAL_CONFIG[\"batch_size\"]\n",
    "        self.lr = OPTIMAL_CONFIG[\"lr\"]\n",
    "        self.lr_warmup = OPTIMAL_CONFIG[\"lr_warmup\"]\n",
    "        self.weight_decay = OPTIMAL_CONFIG[\"weight_decay\"]\n",
    "        self.epochs = OPTIMAL_CONFIG[\"epochs\"]\n",
    "        self.attention_heads = OPTIMAL_CONFIG[\"attention_heads\"]\n",
    "        self.hidden_dim = OPTIMAL_CONFIG[\"hidden_dim\"]\n",
    "        self.gcn_dim = OPTIMAL_CONFIG[\"gcn_dim\"]\n",
    "        \n",
    "        # GCN configuration\n",
    "        self.gcn_type = OPTIMAL_CONFIG[\"gcn_type\"]\n",
    "        self.num_layers = OPTIMAL_CONFIG[\"gcn_layers\"]\n",
    "        self.use_improved_gcn = True\n",
    "        self.use_residual = True\n",
    "        self.use_layer_norm = True\n",
    "        self.use_multi_scale = True\n",
    "        self.use_graph_attention = True\n",
    "        self.use_relative_position = True\n",
    "        self.use_global_context = True\n",
    "        \n",
    "        # Model configuration\n",
    "        self.pretrained_deberta_name = \"microsoft/deberta-v2-xxlarge\"\n",
    "        self.tokenizer_path = \"bert-base-uncased\"\n",
    "        self.max_span_size = 6\n",
    "        self.max_pairs = 800\n",
    "        self.sen_filter_threshold = 0.5\n",
    "        \n",
    "        # Training configuration\n",
    "        self.seed = 42\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.drop_out_rate = 0.3\n",
    "        self.gcn_dropout = 0.1\n",
    "        self.prop_drop = 0.05\n",
    "        \n",
    "        # Sampling configuration\n",
    "        self.neg_entity_count = 50\n",
    "        self.neg_triple_count = 50\n",
    "        self.sampling_processes = 4\n",
    "        self.sampling_limit = 100\n",
    "        \n",
    "        # Logging configuration\n",
    "        self.log_path = \"/kaggle/working/log/\"\n",
    "        self.train_log_iter = 1\n",
    "        self.store_predictions = True\n",
    "        self.store_examples = True\n",
    "        self.example_count = None\n",
    "        \n",
    "        # Other configurations\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cpu = False\n",
    "        self.init_eval = False\n",
    "        self.final_eval = False\n",
    "        self.freeze_transformer = False\n",
    "        self.lowercase = True\n",
    "        self.is_bidirect = True\n",
    "        self.use_gated = False\n",
    "        \n",
    "        # Additional dimensions\n",
    "        self.emb_dim = 1536\n",
    "        self.lstm_layers = 2\n",
    "        self.lstm_dim = 384\n",
    "        self.mem_dim = 768\n",
    "        self.deberta_feature_dim = 1536\n",
    "        self.size_embedding = 25\n",
    "        self.span_generator = \"Max\"\n",
    "        self.pooling = \"avg\"\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(self.log_path, exist_ok=True)\n",
    "        \n",
    "        self.label = self.dataset\n",
    "\n",
    "# Create configuration\n",
    "args = TrainingConfig()\n",
    "print(f\"Training on device: {args.device}\")\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "print(f\"Batch size: {args.batch_size}\")\n",
    "print(f\"Learning rate: {args.lr}\")\n",
    "print(f\"Epochs: {args.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Import Model Components\n",
    "\n",
    "**Note**: Make sure all model files are in the correct directory structure after extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the working directory to Python path\n",
    "sys.path.append('/kaggle/working')\n",
    "\n",
    "# Import D2E2S components\n",
    "try:\n",
    "    from models.D2E2S_Model import D2E2SModel\n",
    "    from models.General import set_seed\n",
    "    from trainer import util, sampling\n",
    "    from trainer.baseTrainer import BaseTrainer\n",
    "    from trainer.entities import Dataset\n",
    "    from trainer.evaluator import Evaluator\n",
    "    from trainer.input_reader import JsonInputReader\n",
    "    from trainer.loss import D2E2SLoss\n",
    "    \n",
    "    print(\"âœ“ All D2E2S components imported successfully\")\nexcept ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"Please ensure all model and trainer files are in the correct directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2E2S_Trainer(BaseTrainer):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\")\n",
    "        self._predictions_path = os.path.join(\n",
    "            self._log_path_predict, \"predicted_%s_epoch_%s.json\"\n",
    "        )\n",
    "        self._examples_path = os.path.join(\n",
    "            self._log_path_predict, \"sample_%s_%s_epoch_%s.html\"\n",
    "        )\n",
    "        os.makedirs(self._log_path_result, exist_ok=True)\n",
    "        os.makedirs(self._log_path_predict, exist_ok=True)\n",
    "        self.max_pair_f1 = 40\n",
    "        self.best_epoch = -1\n",
    "        self.result_path = os.path.join(\n",
    "            self._log_path_result, \"result{}.txt\".format(self.args.max_span_size)\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, args, input_reader_cls, types_path, train_path, test_path):\n",
    "        train_label, test_label = \"train\", \"test\"\n",
    "        \n",
    "        # Create log csv files\n",
    "        self._init_train_logging(train_label)\n",
    "        self._init_eval_logging(test_label)\n",
    "\n",
    "        # Loading data\n",
    "        input_reader = input_reader_cls(\n",
    "            types_path,\n",
    "            self._tokenizer,\n",
    "            args.neg_entity_count,\n",
    "            args.neg_triple_count,\n",
    "            args.max_span_size,\n",
    "        )\n",
    "        input_reader.read({train_label: train_path, test_label: test_path})\n",
    "        train_dataset = input_reader.get_dataset(train_label)\n",
    "\n",
    "        # Preprocess\n",
    "        train_sample_count = train_dataset.sentence_count\n",
    "        updates_epoch = train_sample_count // args.batch_size\n",
    "        updates_total = updates_epoch * args.epochs\n",
    "\n",
    "        print(f\"Dataset: {args.dataset}, Max span size: {args.max_span_size}\")\n",
    "        print(f\"Training samples: {train_sample_count}\")\n",
    "        print(f\"Updates per epoch: {updates_epoch}\")\n",
    "        print(f\"Total updates: {updates_total}\")\n",
    "        \n",
    "        return input_reader, updates_total, updates_epoch\n\n",
    "    def train_model(self, train_path: str, test_path: str, types_path: str, input_reader_cls):\n",
    "        args = self.args\n",
    "\n",
    "        # Set seed\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        train_label, test_label = \"train\", \"test\"\n",
    "        input_reader, updates_total, updates_epoch = self._preprocess(\n",
    "            args, input_reader_cls, types_path, train_path, test_path\n",
    "        )\n",
    "        train_dataset = input_reader.get_dataset(train_label)\n",
    "        test_dataset = input_reader.get_dataset(test_label)\n",
    "\n",
    "        # Load model\n",
    "        config = AutoConfig.from_pretrained(\"microsoft/deberta-v2-xxlarge\")\n",
    "\n",
    "        model = D2E2SModel.from_pretrained(\n",
    "            args.pretrained_deberta_name,\n",
    "            config=config,\n",
    "            cls_token=self._tokenizer.convert_tokens_to_ids(\"[CLS]\"),\n",
    "            sentiment_types=input_reader.sentiment_type_count - 1,\n",
    "            entity_types=input_reader.entity_type_count,\n",
    "            args=args,\n",
    "        )\n",
    "        model.to(args.device)\n",
    "        \n",
    "        print(f\"Model loaded on {args.device}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "        # Create optimizer\n",
    "        optimizer_params = self._get_optimizer_params(model)\n",
    "        optimizer = AdamW(\n",
    "            optimizer_params,\n",
    "            lr=args.lr,\n",
    "            weight_decay=args.weight_decay,\n",
    "            correct_bias=False,\n",
    "        )\n",
    "        \n",
    "        # Create scheduler\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.lr_warmup * updates_total,\n",
    "            num_training_steps=updates_total,\n",
    "        )\n",
    "\n",
    "        # Create loss function\n",
    "        entity_criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        senti_criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        compute_loss = D2E2SLoss(\n",
    "            senti_criterion,\n",
    "            entity_criterion,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            args.max_grad_norm,\n",
    "        )\n",
    "\n",
    "        # Initial evaluation\n",
    "        if args.init_eval:\n",
    "            self._eval(model, test_dataset, input_reader, 0, updates_epoch)\n",
    "\n",
    "        # Training loop\n",
    "        print(f\"\\nStarting training for {args.epochs} epochs...\")\n",
    "        for epoch in range(args.epochs):\n",
    "            print(f\"\\n=== Epoch {epoch + 1}/{args.epochs} ===\")\n",
    "            \n",
    "            # Train epoch\n",
    "            self.train_epoch(\n",
    "                model, compute_loss, optimizer, train_dataset, updates_epoch, epoch\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            if not args.final_eval or (epoch == args.epochs - 1):\n",
    "                self._eval(model, test_dataset, input_reader, epoch + 1, updates_epoch)\n",
    "                \n",
    "        print(f\"\\nðŸŽ‰ Training completed!\")\n",
    "        print(f\"Best F1 score: {self.max_pair_f1:.4f} at epoch {self.best_epoch}\")\n",
    "        \n",
    "        return model\n\n",
    "    def train_epoch(self, model, compute_loss, optimizer, dataset, updates_epoch, epoch):\n",
    "        # Create data loader\n",
    "        dataset.switch_mode(Dataset.TRAIN_MODE)\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.args.sampling_processes,\n",
    "            collate_fn=sampling.collate_fn_padding,\n",
    "        )\n",
    "\n",
    "        model.zero_grad()\n",
    "        iteration = 0\n",
    "        total = dataset.sentence_count // self.args.batch_size\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch in tqdm(data_loader, total=total, desc=f\"Training epoch {epoch + 1}\"):\n",
    "            model.train()\n",
    "            batch = util.to_device(batch, self.args.device)\n",
    "\n",
    "            # Forward step\n",
    "            entity_logits, senti_logits, batch_loss = model(\n",
    "                encodings=batch[\"encodings\"],\n",
    "                context_masks=batch[\"context_masks\"],\n",
    "                entity_masks=batch[\"entity_masks\"],\n",
    "                entity_sizes=batch[\"entity_sizes\"],\n",
    "                sentiments=batch[\"rels\"],\n",
    "                senti_masks=batch[\"senti_masks\"],\n",
    "                adj=batch[\"adj\"],\n",
    "            )\n",
    "\n",
    "            # Compute loss and optimize parameters\n",
    "            epoch_loss = compute_loss.compute(\n",
    "                entity_logits=entity_logits,\n",
    "                senti_logits=senti_logits,\n",
    "                batch_loss=batch_loss,\n",
    "                senti_types=batch[\"senti_types\"],\n",
    "                entity_types=batch[\"entity_types\"],\n",
    "                entity_sample_masks=batch[\"entity_sample_masks\"],\n",
    "                senti_sample_masks=batch[\"senti_sample_masks\"],\n",
    "            )\n",
    "            \n",
    "            epoch_losses.append(epoch_loss)\n",
    "            iteration += 1\n",
    "            global_iteration = epoch * updates_epoch + iteration\n",
    "\n",
    "            if global_iteration % self.args.train_log_iter == 0:\n",
    "                self._log_train(\n",
    "                    optimizer, epoch_loss, epoch, iteration, global_iteration, dataset.label\n",
    "                )\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "        return iteration\n\n",
    "    def _eval(self, model, dataset, input_reader, epoch=0, updates_epoch=0, iteration=0):\n",
    "        # Create evaluator\n",
    "        evaluator = Evaluator(\n",
    "            dataset,\n",
    "            input_reader,\n",
    "            self._tokenizer,\n",
    "            self.args.sen_filter_threshold,\n",
    "            self._predictions_path,\n",
    "            self._examples_path,\n",
    "            self.args.example_count,\n",
    "            epoch,\n",
    "            dataset.label,\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset.switch_mode(Dataset.EVAL_MODE)\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.args.sampling_processes,\n",
    "            collate_fn=sampling.collate_fn_padding,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total = math.ceil(dataset.sentence_count / self.args.batch_size)\n",
    "            \n",
    "            for batch in tqdm(data_loader, total=total, desc=f\"Evaluating epoch {epoch}\"):\n",
    "                batch = util.to_device(batch, self.args.device)\n",
    "\n",
    "                # Run model (forward pass)\n",
    "                result = model(\n",
    "                    encodings=batch[\"encodings\"],\n",
    "                    context_masks=batch[\"context_masks\"],\n",
    "                    entity_masks=batch[\"entity_masks\"],\n",
    "                    entity_sizes=batch[\"entity_sizes\"],\n",
    "                    entity_spans=batch[\"entity_spans\"],\n",
    "                    entity_sample_masks=batch[\"entity_sample_masks\"],\n",
    "                    evaluate=True,\n",
    "                    adj=batch[\"adj\"],\n",
    "                )\n",
    "                entity_clf, senti_clf, rels = result\n",
    "                evaluator.eval_batch(entity_clf, senti_clf, rels, batch)\n",
    "                \n",
    "            global_iteration = epoch * updates_epoch + iteration\n",
    "            ner_eval, senti_eval, senti_nec_eval = evaluator.compute_scores()\n",
    "            \n",
    "            # Log results\n",
    "            self._log_filter_file(ner_eval, senti_eval, evaluator, epoch)\n",
    "            \n",
    "        self._log_eval(\n",
    "            *ner_eval, *senti_eval, *senti_nec_eval,\n",
    "            epoch, iteration, global_iteration, dataset.label\n",
    "        )\n",
    "        \n",
    "        # Print current results\n",
    "        print(f\"Sentiment F1: {senti_eval[2]:.4f}\")\n",
    "        if senti_eval[2] > self.max_pair_f1:\n",
    "            print(f\"ðŸŽ¯ New best F1 score: {senti_eval[2]:.4f}\")\n\n",
    "    def _log_filter_file(self, ner_eval, senti_eval, evaluator, epoch):\n",
    "        f1 = float(senti_eval[2])\n",
    "        if self.max_pair_f1 < f1:\n",
    "            columns = [\"mic_precision\", \"mic_recall\", \"mic_f1_score\", \"mac_precision\", \"mac_recall\", \"mac_f1_score\"]\n",
    "            \n",
    "            senti_dic = {columns[i]: val for i, val in enumerate(senti_eval)}\n",
    "            self.max_pair_f1 = f1\n",
    "            self.best_epoch = epoch\n",
    "            \n",
    "            with open(self.result_path, mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Epoch {epoch}:\\n\")\n",
    "                f.write(f\"Sentiment results: {senti_dic}\\n\\n\")\n",
    "                \n",
    "            # Clean up old predictions\n",
    "            try:\n",
    "                for filename in os.listdir(self._log_path_predict):\n",
    "                    os.remove(os.path.join(self._log_path_predict, filename))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not clean predictions directory: {e}\")\n",
    "                \n",
    "            if self.args.store_predictions:\n",
    "                evaluator.store_predictions()\n",
    "            if self.args.store_examples:\n",
    "                evaluator.store_examples()\n\n",
    "    def _get_optimizer_params(self, model):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_params = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return optimizer_params\n",
    "\n",
    "print(\"âœ“ D2E2S_Trainer class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all required files exist\n",
    "required_files = [\n",
    "    args.dataset_file[\"train\"],\n",
    "    args.dataset_file[\"test\"],\n",
    "    args.dataset_file[\"types_path\"]\n",
    "]\n",
    "\n",
    "print(\"Checking required dataset files:\")\n",
    "all_files_exist = True\n",
    "for file_path in required_files:\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"âœ“ {file_path} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {file_path} - NOT FOUND\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"\\nâœ… All required files are present!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Some required files are missing. Please check your dataset upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = D2E2S_Trainer(args)\n",
    "\n",
    "print(\"ðŸš€ Starting D2E2S training with optimal hyperparameters...\")\n",
    "print(f\"Configuration: {OPTIMAL_CONFIG}\")\n",
    "print(f\"Expected F1 score: ~0.8644 (based on optimization results)\")\n",
    "\n",
    "# Start training\n",
    "trained_model = trainer.train_model(\n",
    "    train_path=args.dataset_file[\"train\"],\n",
    "    test_path=args.dataset_file[\"test\"],\n",
    "    types_path=args.dataset_file[\"types_path\"],\n",
    "    input_reader_cls=JsonInputReader,\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training completed successfully!\")\n",
    "print(f\"Final best F1 score: {trainer.max_pair_f1:.4f} at epoch {trainer.best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "results = {\n",
    "    \"dataset\": args.dataset,\n",
    "    \"best_f1_score\": float(trainer.max_pair_f1),\n",
    "    \"best_epoch\": trainer.best_epoch,\n",
    "    \"total_epochs\": args.epochs,\n",
    "    \"hyperparameters\": OPTIMAL_CONFIG,\n",
    "    \"model_parameters\": sum(p.numel() for p in trained_model.parameters()),\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "with open('/kaggle/working/training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“Š Training Results:\")\n",
    "print(f\"  Dataset: {results['dataset']}\")\n",
    "print(f\"  Best F1 Score: {results['best_f1_score']:.4f}\")\n",
    "print(f\"  Best Epoch: {results['best_epoch']}\")\n",
    "print(f\"  Total Epochs: {results['total_epochs']}\")\n",
    "print(f\"  Model Parameters: {results['model_parameters']:,}\")\n",
    "\n",
    "# Save model checkpoint\n",
    "model_save_path = '/kaggle/working/d2e2s_best_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'config': args,\n",
    "    'results': results\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Model saved to: {model_save_path}\")\n",
    "print(f\"ðŸ“„ Results saved to: /kaggle/working/training_results.json\")\n",
    "print(f\"ðŸ“‹ Detailed logs saved to: {args.log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read training logs if available\n",
    "try:\n",
    "    log_files = [f for f in os.listdir(args.log_path) if f.endswith('.csv')]\n",
    "    if log_files:\n",
    "        print(\"ðŸ“ˆ Training logs found:\")\n",
    "        for log_file in log_files:\n",
    "            print(f\"  - {log_file}\")\n",
    "            \n",
    "        # Try to plot training progress\n",
    "        train_log = None\n",
    "        for log_file in log_files:\n",
    "            if 'train' in log_file.lower():\n",
    "                train_log = pd.read_csv(os.path.join(args.log_path, log_file))\n",
    "                break\n",
    "                \n",
    "        if train_log is not None and 'loss_avg' in train_log.columns:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_log['loss_avg'])\n",
    "            plt.title('Training Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Average Loss')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            if 'lr' in train_log.columns:\n",
    "                plt.plot(train_log['lr'])\n",
    "                plt.title('Learning Rate Schedule')\n",
    "                plt.xlabel('Iteration')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('/kaggle/working/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"ðŸ“Š Training progress plot saved to: /kaggle/working/training_progress.png\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate training plots: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: D2E2S (Dual-channel Enhanced Entity-Sentiment)\")\n",
    "print(f\"Dataset: {args.dataset} (Restaurant Reviews 2015)\")\n",
    "print(f\"Configuration: Hybrid GCN with optimal hyperparameters\")\n",
    "print(f\"Best F1 Score: {trainer.max_pair_f1:.4f}\")\n",
    "print(f\"Achieved at Epoch: {trainer.best_epoch}\")\n",
    "print(f\"Total Training Time: {args.epochs} epochs\")\n",
    "print(f\"Device Used: {args.device}\")\n",
    "print(\"\\nâœ… Training completed successfully!\")\n",
    "print(\"ðŸ“ All outputs saved to /kaggle/working/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### What was accomplished:\n",
    "1. âœ… Installed all required dependencies\n",
    "2. âœ… Set up optimal hyperparameters (F1: 0.8644)\n",
    "3. âœ… Loaded and preprocessed the 15res dataset\n",
    "4. âœ… Trained D2E2S model with Hybrid GCN configuration\n",
    "5. âœ… Saved model checkpoint and training results\n",
    "6. âœ… Generated performance analysis\n",
    "\n",
    "### Output Files:\n",
    "- `d2e2s_best_model.pth` - Trained model checkpoint\n",
    "- `training_results.json` - Training metrics and configuration\n",
    "- `training_progress.png` - Training loss and learning rate plots\n",
    "- `log/` directory - Detailed training logs\n",
    "\n",
    "### Key Results:\n",
    "- **Model**: D2E2S with Hybrid GCN\n",
    "- **Dataset**: 15res (Restaurant Reviews 2015)\n",
    "- **Expected F1**: ~0.8644 (based on hyperparameter optimization)\n",
    "- **Configuration**: Production-ready optimal parameters\n",
    "\n",
    "The model is now ready for inference on aspect-based sentiment analysis tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}